{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import re #Regular Expressions\n",
    "import operator # Majorly for sorting tf-idf dictionary on basis of values\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # For removing stop words, use ntlk.download() to completely install this package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection as cv\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to customize according to use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/aakash/Drive/Dropbox/ML/data/20_newsgroups/\" #Set  this according to your machine\n",
    "class_names = os.listdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use small numbers for first time, articles =  100, classes = 2 , features = 100 \n",
    "max_articles_of_each_class = 1000\n",
    "no_of_classes = 5\n",
    "no_of_features = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caclulating tf-idf metric for articles of selected classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file_text(file):\n",
    "    text = file.read().lower() # Converting all to lowercase as lowercase and uppercase words should be considered same word\n",
    "    text = re.sub('[^A-Za-z ]+', '', text) # Removing non-aplha characters\n",
    "    text = re.sub('\\s+', ' ', text)  # Condense all whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = set()\n",
    "no_of_features_per_class =  no_of_features//no_of_classes\n",
    "# tf = {}\n",
    "# idf = {}\n",
    "selected_classes = []\n",
    "articles_read = {} # To keep track of which all articles were read while extracting features\n",
    "for i in range(0,len(class_names)):\n",
    "    tf = {}\n",
    "    idf = {}\n",
    "    if(i >= no_of_classes):\n",
    "        break\n",
    "    current_class = class_names[i]\n",
    "    \n",
    "    # 2 updates for tracking\n",
    "    selected_classes.append(current_class)\n",
    "    articles_read[current_class] = []\n",
    "    \n",
    "    class_dir = base_dir + current_class\n",
    "    all_articles = os.listdir(class_dir)\n",
    "    for j in range(0,len(all_articles)):\n",
    "        if(j >= max_articles_of_each_class):\n",
    "            break\n",
    "        current_file = class_dir + \"/\" + all_articles[j]\n",
    "        articles_read[current_class].append(all_articles[j]) \n",
    "        file = open(current_file, encoding = \"ISO-8859-1\")\n",
    "        text = preprocess_file_text(file)\n",
    "        file.close() # Always close a file after using it to free up system resources\n",
    "        file_words = text.split()\n",
    "        \n",
    "        # Updating term-frequency dictionary\n",
    "        word_count = collections.Counter(file_words)\n",
    "        for word,freq in word_count.items():\n",
    "            if(word in tf):\n",
    "                tf[word] = tf[word] + freq\n",
    "            else:\n",
    "                tf[word] = freq\n",
    "        \n",
    "        #Updating (inverse document frequency) dictionary\n",
    "        word_set = set(file_words)\n",
    "        for word in word_set:\n",
    "            if(word in idf):\n",
    "                idf[word] = idf[word] + 1\n",
    "            else:\n",
    "                idf[word] = 1\n",
    "    \n",
    "    #Taking equal no. of features from each class\n",
    "    tf_by_idf = {}\n",
    "    for key in tf.keys():\n",
    "        tf_by_idf[key] = tf[key]/idf[key]\n",
    "    for stop_word in stopwords.words(\"english\"):\n",
    "        if(stop_word in tf_by_idf.keys()):\n",
    "            tf_by_idf.pop(stop_word)\n",
    "    tf_by_idf = sorted(tf_by_idf.items(), key=operator.itemgetter(1))\n",
    "    tf_by_idf.reverse()\n",
    "    for i in range(0,no_of_features_per_class):\n",
    "        features.add(tf_by_idf[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataframe by reading the articles again on the basis of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 articles are processed out of 5000 articles\n",
      "1000 articles are processed out of 5000 articles\n",
      "1500 articles are processed out of 5000 articles\n",
      "2000 articles are processed out of 5000 articles\n",
      "2500 articles are processed out of 5000 articles\n",
      "3000 articles are processed out of 5000 articles\n",
      "3500 articles are processed out of 5000 articles\n",
      "4000 articles are processed out of 5000 articles\n",
      "4500 articles are processed out of 5000 articles\n",
      "5000 articles are processed out of 5000 articles\n"
     ]
    }
   ],
   "source": [
    "# It takes time,so be patient (Reduce feature count to reduce this time but score will be affected accordingly)\n",
    "\n",
    "columns = list(features)\n",
    "total_articles_to_process = 0\n",
    "for class_name in articles_read.keys():\n",
    "    total_articles_to_process += len(articles_read[class_name])\n",
    "data = []\n",
    "\n",
    "articles_processed = 0\n",
    "for current_class in selected_classes:\n",
    "    class_dir = base_dir + current_class\n",
    "    for article in articles_read[current_class]:\n",
    "        articles_processed += 1\n",
    "        if(articles_processed%500 == 0):\n",
    "            print(articles_processed,\"articles are processed out of\",total_articles_to_process,\"articles\")\n",
    "        current_file = class_dir + \"/\" + article\n",
    "        file = open(current_file, encoding = \"ISO-8859-1\")\n",
    "        text = preprocess_file_text(file)\n",
    "        file.close() # Always close a file after using it to free up system resources\n",
    "        file_words = text.split()\n",
    "        \n",
    "        word_count = collections.Counter(file_words)\n",
    "        training_data = [0]*(len(columns) + 1) # +1 because last column is of output class\n",
    "        for i in range(0,len(columns)): \n",
    "            feature = columns[i]\n",
    "            if(feature in word_count.keys()):\n",
    "                training_data[i] = word_count[feature]\n",
    "        training_data[-1] = current_class\n",
    "        data.append(training_data)\n",
    "\n",
    "columns.append(\"class\")\n",
    "df = pd.DataFrame(data,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values[:,:-1]\n",
    "Y = df.values[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = cv.train_test_split(X,Y,test_size=0.05,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Inbuilt Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Y_test =  250  and wrong results =  129\n",
      "Score is  0.484\n"
     ]
    }
   ],
   "source": [
    "gnb = naive_bayes.GaussianNB()\n",
    "gnb.fit(X_train,Y_train)\n",
    "Y_pred = gnb.predict(X_test)\n",
    "print(\"Size of Y_test = \",len(Y_test),\" and wrong results = \",(Y_pred != Y_test).sum())\n",
    "print(\"Score is \",gnb.score(X_test,Y_test))\n",
    "#print(gnb.predict_proba(X_test[Y_test != Y_pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using my implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePriorProbabilities(Y):\n",
    "    classes = set(Y)\n",
    "    result = {}\n",
    "    for i in classes:\n",
    "        result[i] = (len(Y[Y==i])/len(Y))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dictionary\n",
    "# Structure of dictionary {class_names : { keys are features + priorProbaility : Value is hash containing mean and variance of feature of those class samples}}\n",
    "def fit(X_train,Y_train,priorProbabilities={}):\n",
    "    result = {}\n",
    "    output_classes = set(Y_train)\n",
    "    if (len(priorProbabilities) != len(output_classes)) :\n",
    "        priorProbabilities = calculatePriorProbabilities(Y_train)\n",
    "    \n",
    "    #This epsilon is added to variance of each feature as zero variance will cause numeric errors\n",
    "    epsilon = 1e-9 * np.var(X_train,axis=0).max()\n",
    "    \n",
    "    for current_class in output_classes:\n",
    "        value = {}\n",
    "        result[current_class] = value\n",
    "        class_samples = (Y_train == current_class)\n",
    "        Y_train_current = Y_train[class_samples]\n",
    "        X_train_current = X_train[class_samples]\n",
    "        for feature in range(0,X_train.shape[-1]):\n",
    "            #Since each feature is a Gaussian Distribution, we need to store mean of those samples only\n",
    "            value[feature] = {}\n",
    "            feature_hash = value[feature]\n",
    "            feature_hash[\"mean\"] = X_train_current[:,feature].mean()\n",
    "            feature_hash[\"var\"] = X_train_current[:,feature].var() + epsilon\n",
    "        value[\"priorProbability\"] = priorProbabilities[current_class]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return log of gaussian probability\n",
    "def logGPUsingMean(mean,variance,x):\n",
    "    logProb = - 0.5 * ((x - mean)**2)/variance # -0.5 because of 2 in denominator of power and - in numerator which were omitted in expression\n",
    "    logProb = logProb - 0.5 * np.log(2*np.pi*variance)\n",
    "    return logProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictClassProbabiltyUsingDictionary(dictionary,X_test_sample):\n",
    "    result = np.log(dictionary[\"priorProbability\"])\n",
    "    for i in range(0,len(dictionary.keys())-1): #-1 because last key is for probabilty, and each key is a feature\n",
    "        logGP = logGPUsingMean(dictionary[i][\"mean\"],dictionary[i][\"var\"],X_test_sample[i])\n",
    "        result = result + (logGP)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictUsingDictionary(dictionary,X_test,priorProbabilities={}):\n",
    "    classes = set(dictionary.keys())\n",
    "    test_samples = X_test.shape[0]\n",
    "    y_pred = [0] * test_samples\n",
    "    \n",
    "    for i in range(0,test_samples):\n",
    "        probabilities = {}\n",
    "        for current_class in classes:\n",
    "            probabilities[current_class] = predictClassProbabiltyUsingDictionary(dictionary[current_class],X_test[i,:])\n",
    "        #print(\"For sample i = \",i,\" probabilities are = \",probabilities)\n",
    "        y_pred[i] = max(probabilities,key=probabilities.get)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Y_test =  250  and wrong results =  129\n"
     ]
    }
   ],
   "source": [
    "Y_pred = predictUsingDictionary(dictionary,X_test)\n",
    "print(\"Size of Y_test = \",len(Y_test),\" and wrong results = \",(Y_pred != Y_test).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations and Doubts\n",
    "\n",
    "### Algo worked better overall with class wise distribution of features\n",
    "### Use log likelihood of classes, otherwise most of probabilities will turn out to be so small that it would eventually become zero and can't be compared for different classes.\n",
    "### Issue occurs if order of size of dataframe becomes more than 10^7. So, how to deal with 20 classes with 1000 articles each if we consider atleast 1000 featues, the size will be of order 2*10^7. Algo will perform much better with around 10^4 feature count. So, for inbuilt NB, use partial_fit method on chunks of data. \n",
    "### For correction in Gaussian Bayes Theorem, we can add epsilon to variance inorder to prevent divison by zero which causes numeric errors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
