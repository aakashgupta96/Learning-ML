{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pydotplus\n",
    "import pandas as pd\n",
    "import sklearn.datasets as Datasets\n",
    "from sklearn import tree\n",
    "from sklearn import model_selection as cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Decision Tree class to use tree for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self):\n",
    "        self.children = {} #If size = 0, implies it is leaf node. Hash is to keep track that which child node corresponds to which label\n",
    "        self.split_feature = None\n",
    "        self.output_class = None # Required when we reach leaf or pure node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeLabelled(column):\n",
    "    mean = column.mean()\n",
    "    for i in range (0,len(column)):\n",
    "        column[i] = int(column[i]>=mean) \n",
    "    return column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifying Iris Dataset to change it to Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = Datasets.load_iris()\n",
    "df = pd.DataFrame(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,X.shape[-1]):\n",
    "    X[:,i] = makeLabelled(X[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = cv.train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing results using sklearn decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_data = tree.export_graphviz(clf,out_file= None)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_pdf(\"irisDecisionTree.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73684210526315785"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(Y):\n",
    "    classes = set(Y)\n",
    "    value = 0\n",
    "    for i in classes:\n",
    "        p = (len(Y[Y==i])/len(Y))\n",
    "        value = value - (p*np.log2(p))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findGainRatio(X,Y,selectedFeature):\n",
    "    differentLabels = set(X[:,selectedFeature])\n",
    "    entropyBeforeSplitting = entropy(Y)\n",
    "    entropyAfterSplitting = 0\n",
    "    splitInfo = 0\n",
    "    for i in differentLabels:\n",
    "        newNodeY = Y[(X[:,selectedFeature] == i)]\n",
    "        weightOfSamples = (len(newNodeY)/len(Y))\n",
    "        entropyAfterSplitting = entropyAfterSplitting + (weightOfSamples*entropy(newNodeY))\n",
    "        splitInfo = splitInfo - (weightOfSamples*np.log2(weightOfSamples))\n",
    "    #print(\"Entropy Before Splitting = \",entropyBeforeSplitting)\n",
    "    #print(\"Entropy After Splitting = \",entropyAfterSplitting)\n",
    "    gain = entropyBeforeSplitting - entropyAfterSplitting\n",
    "    gainRatio = gain#/splitInfo\n",
    "    #print(\"gain is = \",gain)\n",
    "    #print(\"split info is = \",splitInfo)\n",
    "    #print(\"gain Ratio is = \",gainRatio)\n",
    "    return gainRatio    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMajorityClass(Y):\n",
    "    classes = set(Y)\n",
    "    ans = -1\n",
    "    max_value = -1\n",
    "    for i in classes:\n",
    "        if(len(Y[Y==i]) >= max_value):\n",
    "            ans = i\n",
    "            max_value = len(Y[Y==i])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for printing only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printClassification(Y):\n",
    "    classes = set(Y)\n",
    "    for i in classes:\n",
    "        print(\"Count of \",i,\" = \",len(Y[Y==i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDTree(dTree):\n",
    "    if(len(dTree.children.keys()) == 0):\n",
    "        print(\"leaf node with output = \",dTree.output_class)\n",
    "        return\n",
    "    print(\"split on feature = \", dTree.split_feature)\n",
    "    for i in dTree.children.keys():\n",
    "        printDTree(dTree.children[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printSplitsInDFS(X,Y,available_features,used_features=[]):\n",
    "    print(\" \")\n",
    "    printClassification(Y)\n",
    "    print(\"Current Entropy is = \",entropy(Y))\n",
    "    if(available_features == 0 or (entropy(Y) == 0)):\n",
    "        print(\"Reached leaf Node having majority class = \",findMajorityClass(Y))\n",
    "        return\n",
    "    selectedFeature = -1 #representing no feature is selected yet\n",
    "    max_value = -float('inf')\n",
    "    for i in range(0,X.shape[-1]):\n",
    "        if(i in used_features):\n",
    "            continue\n",
    "        value = findGainRatio(X,Y,i)\n",
    "        if(value >= max_value and value >= 0):\n",
    "            selectedFeature = i\n",
    "            max_value = value\n",
    "    if(selectedFeature == -1): \n",
    "        return # as no feature can be used to split\n",
    "    \n",
    "    #If after splitting about selected feature, all samples go to one node only, then splitting is of no use and mark it as leaf node\n",
    "    differentLabels = set(X[:,selectedFeature])\n",
    "    if(len(differentLabels) == 1):\n",
    "        print(\"Reached leaf Node having majority class = \",findMajorityClass(Y))\n",
    "        return\n",
    "    \n",
    "    print(\"Splitting on feature \",selectedFeature)\n",
    "    used_features.append(selectedFeature)\n",
    "    for i in differentLabels:\n",
    "        newDataSamplesIndex = (X[:,selectedFeature] == i)\n",
    "        newX = X[newDataSamplesIndex]\n",
    "        newY = Y[newDataSamplesIndex]\n",
    "        printSplitsInDFS(newX,newY,available_features-1,used_features)\n",
    "    used_features.remove(selectedFeature) # As this feature can be used again in other branches for splitting\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main methods for fitting and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitDecisionTree(X,Y,available_features,used_features=[]):\n",
    "    dTree = DecisionTree()\n",
    "    #Save output class on basis of majority\n",
    "    dTree.output_class = findMajorityClass(Y)\n",
    "    \n",
    "    if(available_features == 0 or (entropy(Y) == 0)):\n",
    "        return dTree\n",
    "    #Finding feature about which we want to split\n",
    "    selectedFeature = -1 #representing no feature is selected yet\n",
    "    max_value = -float('inf')\n",
    "    for i in range(0,X.shape[-1]):\n",
    "        if(i in used_features):\n",
    "            continue\n",
    "        value = findGainRatio(X,Y,i)\n",
    "        if(value >= max_value and value >= 0):\n",
    "            selectedFeature = i\n",
    "            max_value = value\n",
    "    if(selectedFeature == -1):  #Implies this node will be a leaf node as no feature can be used to split\n",
    "        return dTree\n",
    "    \n",
    "    #If after splitting about selected feature, all samples go to one node only, then splitting is of no use and mark it as leaf node\n",
    "    differentLabels = set(X[:,selectedFeature])\n",
    "    if(len(differentLabels) == 1):\n",
    "        return dTree\n",
    "    \n",
    "    used_features.append(selectedFeature) #Marking this feature as used so that it can't be used again in any of its subtrees for splitting.\n",
    "    dTree.split_feature = selectedFeature\n",
    "    \n",
    "    #For each unique value of selected feature, add a node for it.\n",
    "    for i in differentLabels: #If labels are 1,2,3... then children dictionary for key 1, will store corrsponding child subtree\n",
    "        newDataSamplesIndex = (X[:,selectedFeature] == i) #Boolean array\n",
    "        newX = X[newDataSamplesIndex]\n",
    "        newY = Y[newDataSamplesIndex]\n",
    "        dTree.children[i] = fitDecisionTree(newX,newY,available_features-1,used_features)\n",
    "    used_features.remove(selectedFeature) # As this feature can be used again in other subtrees of its parent for splitting\n",
    "    return dTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steps:\n",
    "#1. Check feature to split about using dTree\n",
    "#2. If label of that feature is 'i', then go to corresponding child of that node.\n",
    "#3. Do this recursively until you reach a leaf node.\n",
    "\n",
    "def predictForOneSample(dTree,X_test_sample):\n",
    "    if(len(dTree.children.keys()) == 0):\n",
    "        return dTree.output_class\n",
    "    label = X_test_sample[dTree.split_feature] # Label of feature of test sample about which DTree tells us to take decision\n",
    "    return predictForOneSample(dTree.children[label],X_test_sample)\n",
    "    \n",
    "def predictUsingDecisionTree(dTree,X_test):\n",
    "    Y_pred = np.zeros(X_test.shape[0])\n",
    "    for i in range(0,X_test.shape[0]): #Predict for each sample one by one\n",
    "        Y_pred[i] = predictForOneSample(dTree,X_test[i,:])\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying on iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = fitDecisionTree(X_train,Y_train,X_train.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is =  0.736842105263\n"
     ]
    }
   ],
   "source": [
    "Y_pred = predictUsingDecisionTree(dTree,X_test)\n",
    "print(\"Score is = \",(Y_pred == Y_test).sum()/len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR logic in DFS Manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Count of  0  =  2\n",
      "Count of  1  =  2\n",
      "Current Entropy is =  1.0\n",
      "Splitting on feature  1\n",
      " \n",
      "Count of  0  =  1\n",
      "Count of  1  =  1\n",
      "Current Entropy is =  1.0\n",
      "Splitting on feature  0\n",
      " \n",
      "Count of  0  =  1\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node having majority class =  0\n",
      " \n",
      "Count of  1  =  1\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node having majority class =  1\n",
      " \n",
      "Count of  0  =  1\n",
      "Count of  1  =  1\n",
      "Current Entropy is =  1.0\n",
      "Splitting on feature  0\n",
      " \n",
      "Count of  1  =  1\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node having majority class =  1\n",
      " \n",
      "Count of  0  =  1\n",
      "Current Entropy is =  0.0\n",
      "Reached leaf Node having majority class =  0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([0,1,1,0])\n",
    "printSplitsInDFS(X,Y,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
