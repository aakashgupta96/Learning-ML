{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import re #Regular Expressions\n",
    "import operator # Majorly for sorting tf-idf dictionary on basis of values\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # For removing stop words, use ntlk.download() to completely install this package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection as cv\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to customize according to use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/aakash/Drive/Dropbox/ML/data/20_newsgroups/\" #Set  this according to your machine\n",
    "class_names = os.listdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use small numbers for first time, articles =  100, classes = 2 , features = 100 \n",
    "max_articles_of_each_class = 1000\n",
    "no_of_classes = 10\n",
    "no_of_features = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caclulating tf-idf metric for articles of selected classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file_text(file):\n",
    "    text = file.read().lower() # Converting all to lowercase as lowercase and uppercase words should be considered same word\n",
    "    text = re.sub('[^A-Za-z ]+', '', text) # Removing non-aplha characters\n",
    "    text = re.sub('\\s+', ' ', text)  # Condense all whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "tf = {}\n",
    "idf = {}\n",
    "selected_classes = []\n",
    "articles_read = {} # To keep track of which all articles were read while extracting features\n",
    "for i in range(0,len(class_names)):\n",
    "    if(i >= no_of_classes):\n",
    "        break\n",
    "    current_class = class_names[i]\n",
    "    \n",
    "    # 2 updates for tracking\n",
    "    selected_classes.append(current_class)\n",
    "    articles_read[current_class] = []\n",
    "    \n",
    "    class_dir = base_dir + current_class\n",
    "    all_articles = os.listdir(class_dir)\n",
    "    for j in range(0,len(all_articles)):\n",
    "        if(j >= max_articles_of_each_class):\n",
    "            break\n",
    "        current_file = class_dir + \"/\" + all_articles[j]\n",
    "        articles_read[current_class].append(all_articles[j]) \n",
    "        file = open(current_file, encoding = \"ISO-8859-1\")\n",
    "        text = preprocess_file_text(file)\n",
    "        file.close() # Always close a file after using it to free up system resources\n",
    "        file_words = text.split()\n",
    "        \n",
    "        # Updating term-frequency dictionary\n",
    "        word_count = collections.Counter(file_words)\n",
    "        for word,freq in word_count.items():\n",
    "            if(word in tf):\n",
    "                tf[word] = tf[word] + freq\n",
    "            else:\n",
    "                tf[word] = freq\n",
    "        \n",
    "        #Updating (inverse document frequency) dictionary\n",
    "        word_set = set(file_words)\n",
    "        for word in word_set:\n",
    "            if(word in idf):\n",
    "                idf[word] = idf[word] + 1\n",
    "            else:\n",
    "                idf[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_by_idf = {}\n",
    "for key in tf.keys():\n",
    "    tf_by_idf[key] = tf[key]/idf[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words from data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stop_word in stopwords.words(\"english\"):\n",
    "    if(stop_word in tf_by_idf.keys()):\n",
    "        tf_by_idf.pop(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting top x words with max tf-idf value as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_by_idf = sorted(tf_by_idf.items(), key=operator.itemgetter(1))\n",
    "tf_by_idf.reverse()\n",
    "#tf_by_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = set()\n",
    "for i in range(0,no_of_features):\n",
    "    features.add(tf_by_idf[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataframe by reading the articles again on the basis of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 articles are processed out of 10000 articles\n",
      "1000 articles are processed out of 10000 articles\n",
      "1500 articles are processed out of 10000 articles\n",
      "2000 articles are processed out of 10000 articles\n",
      "2500 articles are processed out of 10000 articles\n",
      "3000 articles are processed out of 10000 articles\n",
      "3500 articles are processed out of 10000 articles\n",
      "4000 articles are processed out of 10000 articles\n",
      "4500 articles are processed out of 10000 articles\n",
      "5000 articles are processed out of 10000 articles\n",
      "5500 articles are processed out of 10000 articles\n",
      "6000 articles are processed out of 10000 articles\n",
      "6500 articles are processed out of 10000 articles\n",
      "7000 articles are processed out of 10000 articles\n",
      "7500 articles are processed out of 10000 articles\n",
      "8000 articles are processed out of 10000 articles\n",
      "8500 articles are processed out of 10000 articles\n",
      "9000 articles are processed out of 10000 articles\n",
      "9500 articles are processed out of 10000 articles\n",
      "10000 articles are processed out of 10000 articles\n"
     ]
    }
   ],
   "source": [
    "# It takes time,so be patient (Reduce feature count to reduce this time but score will be affected accordingly)\n",
    "\n",
    "columns = list(features)\n",
    "total_articles_to_process = 0\n",
    "for class_name in articles_read.keys():\n",
    "    total_articles_to_process += len(articles_read[class_name])\n",
    "data = []\n",
    "\n",
    "articles_processed = 0\n",
    "for current_class in selected_classes:\n",
    "    class_dir = base_dir + current_class\n",
    "    for article in articles_read[current_class]:\n",
    "        articles_processed += 1\n",
    "        if(articles_processed%500 == 0):\n",
    "            print(articles_processed,\"articles are processed out of\",total_articles_to_process,\"articles\")\n",
    "        current_file = class_dir + \"/\" + article\n",
    "        file = open(current_file, encoding = \"ISO-8859-1\")\n",
    "        text = preprocess_file_text(file)\n",
    "        file.close() # Always close a file after using it to free up system resources\n",
    "        file_words = text.split()\n",
    "        \n",
    "        word_count = collections.Counter(file_words)\n",
    "        training_data = [0]*(len(columns) + 1) # +1 because last column is of output class\n",
    "        for i in range(0,len(columns)): \n",
    "            feature = columns[i]\n",
    "            if(feature in word_count.keys()):\n",
    "                training_data[i] = word_count[feature]\n",
    "        training_data[-1] = current_class\n",
    "        data.append(training_data)\n",
    "\n",
    "columns.append(\"class\")\n",
    "df = pd.DataFrame(data,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save created df to csv file\n",
    "#df.to_csv(\"data_\" + str(no_of_classes) + \"classes.csv\",encoding=\"UTF-8\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load df from saved file\n",
    "#df = pd.read_csv(\"data_\" + str(no_of_classes) + \"classes.csv\",index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values[:,:-1]\n",
    "Y = df.values[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = cv.train_test_split(X,Y,test_size=0.05,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = naive_bayes.MultinomialNB(alpha=0.1)\n",
    "mnb.fit(X_train,Y_train)\n",
    "Y_pred = mnb.predict(X_test)\n",
    "print(\"Size of Y_test = \",len(Y_test),\" and wrong results = \",(Y_pred != Y_test).sum())\n",
    "print(\"Score is \",mnb.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = naive_bayes.GaussianNB()\n",
    "gnb.fit(X_train,Y_train)\n",
    "Y_pred = gnb.predict(X_test)\n",
    "print(\"Size of Y_test = \",len(Y_test),\" and wrong results = \",(Y_pred != Y_test).sum())\n",
    "print(\"Score is \",gnb.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using My Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePriorProbabilities(Y):\n",
    "    classes = set(Y)\n",
    "    result = {}\n",
    "    for i in classes:\n",
    "        result[i] = (len(Y[Y==i])/len(Y))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dictionary\n",
    "# Structure of dictionary {class_names : { keys are features + priorProbaility : Value is hash containing mean and variance of feature of those class samples}}\n",
    "def fitGaussianBayes(X_train,Y_train,priorProbabilities={}):\n",
    "    result = {}\n",
    "    output_classes = set(Y_train)\n",
    "    if (len(priorProbabilities) != len(output_classes)) :\n",
    "        priorProbabilities = calculatePriorProbabilities(Y_train)\n",
    "    \n",
    "    #This epsilon is added to variance of each feature as zero variance will cause numeric errors\n",
    "    epsilon = 1e-9 * np.var(X_train,axis=0).max()\n",
    "    \n",
    "    for current_class in output_classes:\n",
    "        value = {}\n",
    "        result[current_class] = value\n",
    "        class_samples = (Y_train == current_class)\n",
    "        Y_train_current = Y_train[class_samples]\n",
    "        X_train_current = X_train[class_samples]\n",
    "        for feature in range(0,X_train.shape[-1]):\n",
    "            #Since each feature is a Gaussian Distribution, we need to store mean and variance of those samples only\n",
    "            value[feature] = {}\n",
    "            feature_hash = value[feature]\n",
    "            feature_hash[\"mean\"] = X_train_current[:,feature].mean()\n",
    "            feature_hash[\"var\"] = X_train_current[:,feature].var() + epsilon\n",
    "        value[\"priorProbability\"] = priorProbabilities[current_class]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns log of gaussian probability\n",
    "def logGPUsingMean(mean,variance,x):\n",
    "# Using commented approach can have cases with (x-mean)**2 having greater value than 700 and thus nr in prob variable will become nearly zero and will take log to inf\n",
    "#     power = -((x - mean)**2)/(2*variance)\n",
    "#     prob = np.exp(power)/np.sqrt(2*np.pi*variance)\n",
    "#     return np.log(prob)\n",
    "    logProb = - 0.5 * ((x - mean)**2)/variance # -0.5 because of 2 in denominator of power and - in numerator which were omitted in expression\n",
    "    logProb = logProb - 0.5 * np.log(2*np.pi*variance)\n",
    "    return logProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictGaussianClassProbabiltyUsingDictionary(dictionary,X_test_sample):\n",
    "    result = np.log(dictionary[\"priorProbability\"])\n",
    "    for i in range(0,len(dictionary.keys())-1): #-1 because last key is for probabilty, and each key is a feature\n",
    "        logGP = logGPUsingMean(dictionary[i][\"mean\"],dictionary[i][\"var\"],X_test_sample[i])\n",
    "        result = result + (logGP)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictGaussianBayes(dictionary,X_test):\n",
    "    classes = set(dictionary.keys())\n",
    "    test_samples = X_test.shape[0]\n",
    "    y_pred = [0] * test_samples\n",
    "    \n",
    "    for i in range(0,test_samples):\n",
    "        probabilities = {}\n",
    "        for current_class in classes:\n",
    "            probabilities[current_class] = predictGaussianClassProbabiltyUsingDictionary(dictionary[current_class],X_test[i,:])\n",
    "        #print(\"For sample i = \",i,\" probabilities are = \",probabilities)\n",
    "        y_pred[i] = max(probabilities,key=probabilities.get)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fitGaussianBayes(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predictGaussianBayes(dictionary,X_test)\n",
    "print(\"Size of Y_test = \",len(Y_test),\" and wrong results = \",(Y_pred != Y_test).sum())\n",
    "print(\"Score is \",(Y_pred == Y_test).sum()/(len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Multinomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dictionary\n",
    "# Structure of dictionary {class_names : { keys are index of features as per df, prior Probaility of class, correctedDenominator for calculating probability : Value for each feature is sum of all frequencies of that feature in training samples of that class.}}\n",
    "def fitMultinomialBayes(X_train,Y_train,priorProbabilities={},correctionFactor=1):\n",
    "    result = {}\n",
    "    output_classes = set(Y_train)\n",
    "    featureCount = X_train.shape[1]\n",
    "    correctionFactor = max(1e-10,correctionFactor) # To prevent absolute zeros\n",
    "    if (len(priorProbabilities) != len(output_classes)) :\n",
    "        priorProbabilities = calculatePriorProbabilities(Y_train)\n",
    "        \n",
    "    for current_class in output_classes:\n",
    "        value = {}\n",
    "        result[current_class] = value\n",
    "        class_samples = (Y_train == current_class)\n",
    "        Y_train_current = Y_train[class_samples]\n",
    "        X_train_current = X_train[class_samples]\n",
    "        value[\"priorProbability\"] = priorProbabilities[current_class]\n",
    "        value[\"correctedDr\"] = X_train_current.sum() + (correctionFactor*featureCount)# Sum of frequencies of all words that appear in the articles belonging to current class\n",
    "        for feature in range(0,featureCount):\n",
    "            value[feature] = X_train_current[:,feature].sum() + correctionFactor\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logMultinomialProbability(nr,dr,X):\n",
    "    return X*np.log((nr/dr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictMultinomialClassProbabiltyUsingDictionary(dictionary,X_test_sample):\n",
    "    result = np.log(dictionary[\"priorProbability\"])\n",
    "    dr = dictionary[\"correctedDr\"]\n",
    "    for i in range(0,len(dictionary.keys())-2): #-2 because 2 keys are for priorProbabilty and correctedDenominator and rest of the keys are features\n",
    "        logMP = logMultinomialProbability(dictionary[i],dr,X_test_sample[i])\n",
    "        result = result + (logMP)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictMultinomialBayes(dictionary,X_test):\n",
    "    classes = set(dictionary.keys())\n",
    "    test_samples = X_test.shape[0]\n",
    "    y_pred = [0] * test_samples\n",
    "    \n",
    "    for i in range(0,test_samples):\n",
    "        probabilities = {}\n",
    "        for current_class in classes:\n",
    "            probabilities[current_class] = predictMultinomialClassProbabiltyUsingDictionary(dictionary[current_class],X_test[i,:])\n",
    "        #print(\"For sample i = \",i,\" probabilities are = \",probabilities)\n",
    "        y_pred[i] = max(probabilities,key=probabilities.get)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = fitMultinomialBayes(X_train,Y_train,correctionFactor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = predictMultinomialBayes(dictionary,X_test)\n",
    "print(\"Size of Y_test = \",len(Y_test),\" and wrong results = \",(Y_pred != Y_test).sum())\n",
    "print(\"Score is \",(Y_pred == Y_test).sum()/(len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "### Use log likelihood of classes, otherwise most of probabilities will turn out to be so small that it would eventually become zero and can't be compared for different classes.\n",
    "### Issue occurs if order of size of dataframe becomes more than 10^7. So, how to deal with 20 classes with 1000 articles each if we consider atleast 1000 featues, the size will be of order 2*10^7. Algo will perform much better with around 10^4 feature count. So, for inbuilt NB, use partial_fit method on chunks of data. \n",
    "### For correction in Gaussian Bayes Theorem, we can add epsilon to variance inorder to prevent divison by zero which causes numeric errors. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
