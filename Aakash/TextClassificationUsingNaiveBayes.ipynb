{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import re #Regular Expressions\n",
    "import operator # Majorly for sorting tf-idf dictionary on basis of values\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # For removing stop words, use ntlk.download() to completely install this package\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection as cv\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters to customize according to use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/aakash/Drive/Dropbox/ML/data/20_newsgroups/\" #Set  this according to your machine\n",
    "class_names = os.listdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use small numbers for first time, articles =  100, classes = 2 , features = 100 \n",
    "max_articles_of_each_class = 1000\n",
    "no_of_classes = 2\n",
    "no_of_features = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caclulating tf-idf metric for articles of selected classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file_text(file):\n",
    "    text = file.read().lower() # Converting all to lowercase as lowercase and uppercase words should be considered same word\n",
    "    text = re.sub('[^A-Za-z ]+', '', text) # Removing non-aplha characters\n",
    "    text = re.sub('\\s+', ' ', text)  # Condense all whitespace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "tf = {}\n",
    "idf = {}\n",
    "selected_classes = []\n",
    "articles_read = {} # To keep track of which all articles were read while extracting features\n",
    "for i in range(0,len(class_names)):\n",
    "    if(i >= no_of_classes):\n",
    "        break\n",
    "    current_class = class_names[i]\n",
    "    \n",
    "    # 2 updates for tracking\n",
    "    selected_classes.append(current_class)\n",
    "    articles_read[current_class] = []\n",
    "    \n",
    "    class_dir = base_dir + current_class\n",
    "    all_articles = os.listdir(class_dir)\n",
    "    for j in range(0,len(all_articles)):\n",
    "        if(j >= max_articles_of_each_class):\n",
    "            break\n",
    "        current_file = class_dir + \"/\" + all_articles[j]\n",
    "        articles_read[current_class].append(all_articles[j]) \n",
    "        file = open(current_file, encoding = \"ISO-8859-1\")\n",
    "        text = preprocess_file_text(file)\n",
    "        file.close() # Always close a file after using it to free up system resources\n",
    "        file_words = text.split()\n",
    "        \n",
    "        # Updating term-frequency dictionary\n",
    "        word_count = collections.Counter(file_words)\n",
    "        for word,freq in word_count.items():\n",
    "            if(word in tf):\n",
    "                tf[word] = tf[word] + freq\n",
    "            else:\n",
    "                tf[word] = freq\n",
    "        \n",
    "        #Updating (inverse document frequency) dictionary\n",
    "        word_set = set(file_words)\n",
    "        for word in word_set:\n",
    "            if(word in idf):\n",
    "                idf[word] = idf[word] + 1\n",
    "            else:\n",
    "                idf[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_by_idf = {}\n",
    "for key in tf.keys():\n",
    "    tf_by_idf[key] = tf[key]/idf[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words from data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stop_word in stopwords.words(\"english\"):\n",
    "    if(stop_word in tf_by_idf.keys()):\n",
    "        tf_by_idf.pop(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting top x words with max tf-idf value as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_by_idf = sorted(tf_by_idf.items(), key=operator.itemgetter(1))\n",
    "tf_by_idf.reverse()\n",
    "#tf_by_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = set()\n",
    "for i in range(0,no_of_features):\n",
    "    features.add(tf_by_idf[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataframe by reading the articles again on the basis of selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 articles are processed out of 2000 articles\n",
      "1000 articles are processed out of 2000 articles\n",
      "1500 articles are processed out of 2000 articles\n",
      "2000 articles are processed out of 2000 articles\n"
     ]
    }
   ],
   "source": [
    "# It takes time,so be patient (Reduce feature count to reduce this time but score will be affected accordingly)\n",
    "\n",
    "columns = list(features)\n",
    "total_articles_to_process = 0\n",
    "for class_name in articles_read.keys():\n",
    "    total_articles_to_process += len(articles_read[class_name])\n",
    "data = []\n",
    "\n",
    "articles_processed = 0\n",
    "for current_class in selected_classes:\n",
    "    class_dir = base_dir + current_class\n",
    "    for article in articles_read[current_class]:\n",
    "        articles_processed += 1\n",
    "        if(articles_processed%500 == 0):\n",
    "            print(articles_processed,\"articles are processed out of\",total_articles_to_process,\"articles\")\n",
    "        current_file = class_dir + \"/\" + article\n",
    "        file = open(current_file, encoding = \"ISO-8859-1\")\n",
    "        text = preprocess_file_text(file)\n",
    "        file.close() # Always close a file after using it to free up system resources\n",
    "        file_words = text.split()\n",
    "        \n",
    "        word_count = collections.Counter(file_words)\n",
    "        training_data = [0]*(len(columns) + 1) # +1 because last column is of output class\n",
    "        for i in range(0,len(columns)): \n",
    "            feature = columns[i]\n",
    "            if(feature in word_count.keys()):\n",
    "                training_data[i] = word_count[feature]\n",
    "        training_data[-1] = current_class\n",
    "        data.append(training_data)\n",
    "\n",
    "columns.append(\"class\")\n",
    "df = pd.DataFrame(data,columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"data.csv\",encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values[:,:-1]\n",
    "Y = df.values[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = cv.train_test_split(X,Y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Inbuilt Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Y_test =  500  and wrong results =  15\n",
      "Score is  0.97\n"
     ]
    }
   ],
   "source": [
    "gnb = naive_bayes.GaussianNB()\n",
    "gnb.fit(X_train,Y_train)\n",
    "Y_pred = gnb.predict(X_test)\n",
    "print(\"Size of Y_test = \",len(Y_test),\" and wrong results = \",(Y_pred != Y_test).sum())\n",
    "print(\"Score is \",gnb.score(X_test,Y_test))\n",
    "#print(gnb.predict_proba(X_test[Y_test != Y_pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using my implementation (Incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePriorProbabilities(Y):\n",
    "    classes = set(Y)\n",
    "    result = {}\n",
    "    for i in classes:\n",
    "        result[i] = (len(Y[Y==i])/len(Y))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naiveProbability(priorProbability,current_class,X_train,Y_train,X_test_sample):\n",
    "    result = priorProbability\n",
    "    #Modifying X_train for current class only\n",
    "    class_samples = (Y_train == current_class)\n",
    "    Y_train = Y_train[class_samples]\n",
    "    X_train = X_train[class_samples]\n",
    "    dr = len(Y_train)\n",
    "    for i in range(0,X_train.shape[-1]):\n",
    "        nr = len(X_train[X_train[:,i]==X_test_sample[i]])\n",
    "        result = result * (nr/dr)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Labelled(non-continous) Features only\n",
    "def naiveBayesPredict(X_train,Y_train,X_test,priorProbabilities={}):\n",
    "    classes = set(Y_train)\n",
    "    test_samples = X_test.shape[0]\n",
    "    y_pred = np.zeros(test_samples)\n",
    "    \n",
    "    #Assuming this condition is sufficient\n",
    "    if (len(priorProbabilities) == 0) :\n",
    "        priorProbabilities = calculatePriorProbabilities(Y_train)\n",
    "    #print(priorProbabilities)\n",
    "    for i in range(0,test_samples):\n",
    "        probabilities = {}\n",
    "        for current_class in classes:\n",
    "            probabilities[current_class] = naiveProbability(priorProbabilities[current_class],current_class,X_train,Y_train,X_test[i,:])\n",
    "        #print(\"For sample\",X_test[i,:],\" probabilities are = \",probabilities)\n",
    "        y_pred[i] = max(probabilities,key=probabilities.get)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
